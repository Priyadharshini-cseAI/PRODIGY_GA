{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXjZ3srHWxvzjGrb++Kbw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyadharshini-cseAI/PRODIGY_GA/blob/main/PRODIGY_GA_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "hUaJXVTjPEg8",
        "outputId": "f06827b4-316f-4c4a-8006-d5631bb27199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and tokenizing dataset...\n",
            "Setting up training arguments...\n",
            "Initializing Trainer...\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to ./results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Text Generation ---\n",
            "Prompt: Once upon a time\n",
            "\n",
            "Generated Text:\n",
            "Once upon a time, you will be able to see the world through your eyes.\n",
            "\n",
            "TheWorld is a game that is designed to be played in a way that you can play with your friends. It is also designed for the player to play. You can also play it with friends and play together. The game is for you to learn and learn.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"gpt2\"\n",
        "OUTPUT_DIR = \"./results\"\n",
        "TRAIN_FILE = \"train_data.txt\"\n",
        "NUM_TRAIN_EPOCHS = 1\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
        "BLOCK_SIZE = 32\n",
        "SAVE_STEPS = 50\n",
        "SAVE_TOTAL_LIMIT = 2\n",
        "\n",
        "\n",
        "def create_sample_train_file():\n",
        "    \"\"\"Creates a dummy training file if it doesn't exist.\"\"\"\n",
        "    if not os.path.exists(TRAIN_FILE):\n",
        "        print(f\"'{TRAIN_FILE}' not found. Creating sample training data...\")\n",
        "        with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"Hello, this is a sample sentence for training.\\n\")\n",
        "            f.write(\"GPT-2 is a powerful language model.\\n\")\n",
        "            f.write(\"Fine-tuning helps adapt it to specific tasks.\\n\")\n",
        "            f.write(\"This is another example line of text.\\n\")\n",
        "            f.write(\"The model will learn from these examples.\\n\")\n",
        "        print(\"Sample training file created.\")\n",
        "\n",
        "\n",
        "def load_and_tokenize_dataset(file_path, tokenizer):\n",
        "    \"\"\"Loads and tokenizes text dataset using ðŸ¤— datasets.\"\"\"\n",
        "    dataset = load_dataset(\"text\", data_files=file_path)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=BLOCK_SIZE\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"]\n",
        "    )\n",
        "\n",
        "    return tokenized_dataset[\"train\"]\n",
        "\n",
        "\n",
        "def main():\n",
        "    create_sample_train_file()\n",
        "\n",
        "    print(f\"Loading tokenizer and model: {MODEL_NAME}\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    print(\"Loading and tokenizing dataset...\")\n",
        "    train_dataset = load_and_tokenize_dataset(TRAIN_FILE, tokenizer)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        save_steps=SAVE_STEPS,\n",
        "        save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "        logging_steps=10,\n",
        "        logging_dir=\"./logs\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    print(\"Initializing Trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"Saving model to {OUTPUT_DIR}\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    print(\"\\n--- Text Generation ---\")\n",
        "    prompt = \"Once upon a time\"\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}